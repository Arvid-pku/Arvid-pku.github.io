<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Animal_Farm</title>
    <url>/2020/03/21/Animal-Farm/</url>
    <content><![CDATA[<p>history vs NOW and future</p>
<a id="more"></a>

<p>一口气读完了动物庄园，收获了跟1984相似而又不同的感觉</p>
<h5 id="为了奥威尔"><a href="#为了奥威尔" class="headerlink" title="为了奥威尔"></a>为了奥威尔</h5><p>一个是预言，一个是写实；一个是极尽夸张之能事的预言，一个是暗藏嘲讽与惋惜的写实。相似的是，都是那么的不稽，同时又那么的真实，具有启发思考。</p>
<p>一开始没有去想暗讽的是苏俄的历史，而是先入为主地去对照中国的脚步。结果发现是如此的吻合，惊为天人，以为奥威尔在建国前便预知到了中国的发展和挫折。</p>
<p>后来发现是描写苏俄之后，更让人深思，原来中国与苏联的发展是如此的相似，既有前车之鉴，为何还重蹈覆辙？所幸没有完全一致。</p>
<h5 id="书与史"><a href="#书与史" class="headerlink" title="书与史"></a>书与史</h5><p>再来谈书中的一些角色，让我印象深刻。</p>
<p>无论什么时代，</p>
<p>总有一匹勤勤恳恳而又盲目愚忠的马。不愿相信自己价值观的错误，也不愿去面对，不敢去面对。</p>
<p>无论什么时代，</p>
<p>总需要一只说好话的乌鸦。即使你在受苦受难时对此咬牙切齿地鄙夷，但当掌权后，总是会忘掉之前的厌恶，转而十分享受。这是权力和本性的化学作用。</p>
<p>无论什么时代，</p>
<p>总有投机者，总有阿谀者，总有作伥者。</p>
<p>总有牺牲者，总有梦想者，总有清明者。</p>
<p>当七诫，逐渐在后面加了限定，增加了一条条的“除了……”，“只要……”，“如果……”——我发现，只要掌权者不受限制，那么他就是为所欲为的，他会有各种理由来达成目的，而且不乏阿谀者和支持者。即使在如今也是。似乎只能倚靠掌权者的自律与信仰了吗？我不知道如何是好。</p>
<p>直到最后变成了</p>
<blockquote>
<p>所有动物一律平等，但有些动物比其他动物更加平等</p>
</blockquote>
<p>这难道不是可笑的吗？</p>
<p>这是一个社会逐渐失去初心的故事。当老实的大众不觉醒，觉醒的人们被残害，剩下的只有渣滓，只有耀武扬威不可一世。这是劣币驱逐良币的故事。</p>
<p>又想起了《1984》中的一句口号：</p>
<blockquote>
<p>战争即和平</p>
<p>自由即奴役</p>
<p>无知即力量</p>
</blockquote>
<p>大概，这就是最终的归宿吧</p>
<p>举杯，遥祝那只穿着衣服的猪。</p>
<h5 id="篇后"><a href="#篇后" class="headerlink" title="篇后"></a>篇后</h5><p>本来想在微博上发一些关于《动物庄园》的感想，结果发现这四个词是敏感词，此处 “&amp;”  代表了我的心情，呵呵。</p>
<p>最近赶一些ddl，效果不佳，明日再战</p>
<p>看完了余罪，也是无脑并痛苦着，不写感想了，演的还好，比较契合。</p>
<p>准备读一下《美丽新世界》</p>
<p>为了明天</p>
<p>深夜</p>
<p>sleaking</p>
<p>2020.3.21.1:02</p>
<p>ard</p>
]]></content>
      <categories>
        <category>reviews</category>
      </categories>
      <tags>
        <tag>books</tag>
      </tags>
  </entry>
  <entry>
    <title>where_is_the_time</title>
    <url>/2020/02/26/where-is-the-time/</url>
    <content><![CDATA[<p>time is life, and we are all changing in it.</p>
<a id="more"></a>

<h5 id="需要友谊"><a href="#需要友谊" class="headerlink" title="需要友谊"></a>需要友谊</h5><p>多么羡慕、也多么幸福。</p>
<p>羡慕他们有无话不谈，可打可闹可托付的朋友；也幸福于自己回顾走过的道路，有人相伴，有可以严肃交流，有可以相互勉励，有可以随时打扰，有可以打打闹闹的朋友。</p>
<p>这个时代，需要静思，需要独处，同时也需要陪伴，需要合作。</p>
<p>我们需要一个对自己真诚，愿意损失他自己来对我们付出的人，他会让我们感动，让我们充满希望。同时我们也需要一个愿意为他付出，真心相待的他，会让我们赤诚，让我们感受灵魂。</p>
<h5 id="一切在改变"><a href="#一切在改变" class="headerlink" title="一切在改变"></a>一切在改变</h5><p>无论什么东西，都在时间这个坐标轴上不逆的向前。一切都在改变，包括亲人朋友，也包括亲情友谊。有些东西是愈久弥坚，愈久弥浓，有些则是愈久弥淡。</p>
<p>无论是《老友记》的各自长大，还是爱情公寓的圆满结局，朋友在身边，但朋友也在变化。</p>
<p>你我也在变化，一切都在改变，变浓变淡。</p>
<h5 id="在不知不觉"><a href="#在不知不觉" class="headerlink" title="在不知不觉"></a>在不知不觉</h5><p>他们的时光不知不觉的溜走了</p>
<p>我们的时光就在看他们的时光中不知不觉的溜走了。</p>
<h5 id="行装"><a href="#行装" class="headerlink" title="行装"></a>行装</h5><p>带上什么。</p>
<p>之前似乎太不从容，留恋怀念是好事，但也不要把自己太禁锢在过去，一些东西还是要忘却，要学会给自己剪枝。</p>
<p>轻装上阵。</p>
<h5 id="一些其他"><a href="#一些其他" class="headerlink" title="一些其他"></a>一些其他</h5><p>暑假看完老友记的时候，就有一种关于时光关于友情的感动。这次作为一个半路出家而且极不虔诚的爱情公寓粉丝几乎全程五倍速看完了爱情公寓，而且一开始是及其看不上，单纯看个乐子。可是慢慢的也被感动了，可能我本来就是一个极易被感动被灌输被洗脑被影响的人吧。最后几集是慢慢看的，也被一些细节所打动，比如大力与张伟的两个吻，大家的真心付出之类的。</p>
<p>同时也感受到了演员和导演希望这部剧成为他们自己作品的那种倔强，当然一部剧肯定会带有演员和导演的温度，他们的时光也浸在里面。</p>
<p>看了美国故事也没有写影评。给我的感觉只有悲伤和沧桑。我有一个看完之后凭自己的感觉去写一些感受，写完之后再去看很长时间影评的习惯。一是担心被他人干扰，二是想及时写下最新鲜的感受，三是想继续沉浸在那个世界中。</p>
<blockquote>
<p>晚上滑水摸鱼，看了好多OIer的博客，里面有他们的喜怒哀乐，也有竞赛经历<br>感到了那是一群真正热爱的人，一群为了未来拼搏的人，肃然起敬</p>
<p>之前和今天都无意中遇到了大学同学的网上博客，生活中普普通通的同学，在网上却有这么细腻的心思与经历，感触</p>
<p>前几天也想做一个输出者，做一个有读者意识去写作的人，于是想建一个博客或者网站<br>尝试了csdn，可惜上面也只记录自己的题解和copy一些知识点，并没有很在意庄重<br>也有博客园，简书的账号，也都是浅尝辄止<br>也用github搭建了一个自己的网站，也只是在上面写了几句玩笑话<br>也用Google的账号登录了Google的博客，也写了一篇就戛然而止<br>也在寒假的时候申请了微信公众号，也写了几篇过瘾，也一度热衷于推广，可是下半年就冷落了<br>看到chj的公众号的正式的感觉，那种读者意识，很触动，于是想认真写，可是总觉得之前的不庄重已经沾染了那个号，已经成了比较私人的表达<br>也有bilibili，看着wfy一直致力于dhy，还有xzy也有几个几万加的视频，也很羡慕，也上传了几个视频，仅此而已，觉得自己不适合视频的创作，也更喜欢文字<br>也有知乎，在上面也答了几道题，终究没有很认真负责的对待，也没有耐心去输出，更习惯于输入里面的大海的知识，也觉得自己的回答会被淹没<br>也有github的账号，也花了一上午去探索，发现真的是宝库，可是只学了一些皮毛，上面的创作也需要知识与技术，还要努力<br>昨天上课在jzd的怂恿下登录了许久未登录的微博，今天也看了 设计 的这个主题，感觉很棒，像一个热热闹闹的集市一样，与知乎不一样的风格，但也有很多值得阅读的东西<br>前几天也看了medium，上面优质的英文文章，让我很喜欢，很钦佩，但终究不是干这一行的，也就自己看看吧，没有时间精力去写作<br>这几天一直对上星期看到的 小土刀 的博客很感慨，也很喜欢他的那种分类：巴别塔，好望角，不周山，燕子集……然后又看了他的经历，中山大学，CMU，自律，读书，锻炼，早起……是我理想中的样子啊，简直成了我的偶像，向他学习<br>今天看那些回忆录的时候，知道了hexo这个博客平台，上面有我很喜欢的风格，但是步骤稍显繁琐，等期末之后仔细看看，立下flag，在上面搭一个博客，面向读者去分享<br>于是，我之后在知乎上搜索(不再阅读) ，在微博上消遣和记录小灵感，在hexo上写作，同时择取一部分到公众号上，日常记日记，在csdn上收集和记录做题流水<br>之后还要探索与努力github，也要尝试多用stack overflow</p>
<p>简单的梳理了一下，也算从一个角度总结了我的阅读与经历与期望</p>
<p>至此</p>
</blockquote>
<p>上面是我之前写下的建这个博客的目的。</p>
<p>但是发现自己还是不太擅长面向读者去写作，总是倾向于一种倾诉，于是显得零乱而随便——就像这次上面几段都很短，而这个随意写的“一些其他”就很长。</p>
<p>OK</p>
<p>就这些吧</p>
<p>截然而止？~~</p>
<p>ard</p>
<p>夜</p>
<p>2020.2.26</p>
<p>22：37</p>
]]></content>
      <categories>
        <category>reviews</category>
      </categories>
      <tags>
        <tag>films</tag>
      </tags>
  </entry>
  <entry>
    <title>Bye_my_winter</title>
    <url>/2020/02/15/Bye-my-winter/</url>
    <content><![CDATA[<p>Everything happens for a reason.</p>
<a id="more"></a>

<p>后天就开学了，虽然仍然会在家里。是个时候转变思想，像个松鼠抖开惺忪，跳上枝丫，去透过薄雾，静静等待第一缕春光，享受这还寒乍暖。</p>
<p>这个假期回来的很早，因为计划了很多事情。但是被这没有预料到的疫情打乱了不少。</p>
<p>这个假期玩了不少游戏，看了不少up主的视频，还美其名曰 体验与融入。</p>
<p>到了最后才明白，这些不适合我，应该纯粹。</p>
<p>虽然浑浑噩噩，但也走马观花了很多之前想学习的东西——看了一半李航的《机器学习方法》，学习了里面的感知机、k邻近法、朴素贝叶斯、决策树……，之后发现闷着脑袋看书效果不佳，还是后来为了应用去查询学习比较好；又复习了一遍python，发现暑假里学的只剩下个印象；学习了莫烦的pytorch，numpy，pandas，git，Linux，sklearn，tkinter，matplotlib，爬虫。当然都是仅仅入门而已。</p>
<p>之后写了一下文本分类的任务，学习了word2vec、tf-idf和逻辑斯谛回归、支持向量机、提升方法。进行了代码实现。</p>
<p>此间还看了《乡土中国》，《哈利波特与被诅咒的孩子》、《查令十字街84号》，《云破处》和半本《霍乱时期的爱情》，半本《天龙八部》，电影看了《少林足球》、《一条狗的使命》、《疯狂原始人》、《囧妈》、《美国故事》，《时光慢递》，也看了几集《生活大爆炸》，当然也浪费了很多时间在 朱一旦、何同学、老番茄、家有儿女、三国杀、pvz、爱情公寓的身上。在此“忏悔”，不过应该还是会我行我素。。因为这些碎片时间的消耗，以及心中那种焦虑感紧迫感，导致我这个假期的电影和书看得不多，惋惜。这个过程中，我春节过后开始坚持背单词，也一直在更新博客，写一些书评、影评、感想和刚刚发的第一篇技术性文章~继续坚持！</p>
<p>打算接下来的这几天学习文本生成。</p>
<p>加油，新的学期~</p>
<p>ard</p>
<p>于午后</p>
]]></content>
      <categories>
        <category>summary</category>
      </categories>
      <tags>
        <tag>vacation</tag>
      </tags>
  </entry>
  <entry>
    <title>text_classification_by_sklearn</title>
    <url>/2020/02/13/text-classification-by-sklearn/</url>
    <content><![CDATA[<p>text-classification by python using logistics regression, SVM, Boosting and using tf-idf, word2vec.</p>
<p>python module: sklearn and so on.</p>
<p>python: 3.7.3</p>
<a id="more"></a>

<p>这是我第一次写出完整的NLP项目的代码。之前在理论上花费了太多时间，当时感觉很空洞，可能是因为没上手进行实践吧。实践起来才发现，那些统计理论知识的重要性。过程中出现了很多bug，其中：有的是因为理论没掌握好，有的是不知道的一些工具，也有的是一些毫无意义的但是却很耗时的bug（比如没有将路径都设成英文）</p>
<p>我使用了Amazon的评论的数据库，其中:</p>
<p><img src="D:%5CBlog%5Csource_posts%5Ctext-classification-by-sklearn%5Cdataset.png" alt=""></p>
<p>tag文件的每一行是一句评论，对应的lab是五级制的评分。分为训练集测试集之类的。其中标号为1和2的文件是我自己切割来训练用的，原因是源文件太大，同时也不够干净，有很多非英文字母，比如希腊文等，还有空行之类的，我通过报错边写边整改了大概一万行来训练。</p>
<p>下面写一下我的过程和自己的理解（初学。。请指教）</p>
<p>首先我学习了莫烦的sklearn教程，大致了解了一下sklearn的使用。之后针对文本分类这个具体的项目我去学习了对应的知识。</p>
<h5 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h5><p>接下来我学习了TF-IDF模型和word2vec模型来将文本向量化。其中：</p>
<p>TF-IDF模型每一维是所有文档中的出现的单词，它兼顾了词在研究文本中出现的频数和词的特殊性——通过词频和出现此词的文档数来限制。似乎计算方法不一，但是都考虑了两者，并用log函数来进行处理，并且加一来防止某些项为0。以及知道了在sklearn包中的使用。</p>
<p>word2vec模型是一种分布编码方式，将词语映射到固定长度的向量。我浏览了一遍代码的实现，大致是先建立词汇表，然后通过CBOW或SkipGram方法进行具体训练。训练过程中都用到了Huffman树，然后预测并比较，来更新词向量。</p>
<p>我一开始认为这两种模型本质不太一样，TF-IDF可以单纯直接计算出来，而word2vec需要进行训练，通过误差修正，逐步逼近得到最后结果。但之后又觉得，这两种都是提供了一种数学标准来表示词，只不过word2vec不能直接通过简单计算得到，需要达到误差的极小点，所以本质没有什么不同。</p>
<h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><p>知道了逻辑斯蒂回归（logistic regression）的数学分布，概率模型，以及用极大似然估计来学习（参见《统计学习方法》李航）。</p>
<p>同样在这本书学习了支持向量机，其中重点学习了线性支持向量机。学习了函数间隔，几何间隔，将间隔最大化转化为凸优化问题的数学原理，及将硬间隔转化为软间隔的松弛变量、惩罚系数。</p>
<p>学习了boosting的方法，大致知道了其中的道理，强可学习&amp;弱可学习。重点看了Ababoosting的方法，学习了其中调整权重和进行组合的数学理论。</p>
<p>（上面三段是我学一个写一个这样来的，所以可能不通顺）</p>
<h5 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h5><h6 id="logistic-TF-IDF"><a href="#logistic-TF-IDF" class="headerlink" title="logistic+TF-IDF"></a>logistic+TF-IDF</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection, preprocessing, linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line"><span class="keyword">import</span> sys </span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处是我因为在终端输出的时候，由于文件太大不能完全输出有省略，所以这样设置</span></span><br><span class="line"><span class="comment"># 但是并没有起作用，希望大家指教一下</span></span><br><span class="line"><span class="string">""" pandas.set_option('display.max_columns',1000000)</span></span><br><span class="line"><span class="string">pandas.set_option('display.width', 10000000)</span></span><br><span class="line"><span class="string">pandas.set_option('display.max_colwidth',1000000)</span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line"><span class="comment"># 下文是训练集和测试集，我没有交叉验证之类的，所以没在训练中用另一组文件</span></span><br><span class="line">data_X = pandas.read_csv(<span class="string">'F:/tmp/amazon/train1.tgt'</span>, sep = <span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line">data_y = pandas.read_csv(<span class="string">'F:/tmp/amazon/train1.lab'</span>, sep = <span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line">test_X = pandas.read_csv(<span class="string">'F:/tmp/amazon/test.tgt'</span>, sep = <span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line">test_y = pandas.read_csv(<span class="string">'F:/tmp/amazon/test.lab'</span>, sep = <span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为编码报错，所以卑微的笨拙重定向输出一下</span></span><br><span class="line"><span class="comment"># 报错为：UnicodeEncodeError: 'gbk' codec can't encode character '\xae' in position 164128: illegal multibyte sequence</span></span><br><span class="line"><span class="comment"># 网上的解答是针对str的，我的是列表，最后也没有解决。。。只能手动修改了</span></span><br><span class="line"><span class="comment"># 按网上解答后的报错是：np.nan is an invalid document, expected byte or unicode string</span></span><br><span class="line"><span class="comment"># 和 empty vocabulary; perhaps the documents only contain stop words</span></span><br><span class="line"><span class="comment"># sys.stdout = open('F:/tmp/amazon/train2.tgt', 'w')</span></span><br><span class="line"><span class="string">""" </span></span><br><span class="line"><span class="string"># 此处报错是：tfidf.fit_transform:'int' object has no attribute 'lower'</span></span><br><span class="line"><span class="string"># 是因为我传入的是pandas读出的dataframe？</span></span><br><span class="line"><span class="string"># 最后的解决方法是笨拙的转换成np，array，变成了二维的array，然后再reshape</span></span><br><span class="line"><span class="string">tmp = np.array(test_X)</span></span><br><span class="line"><span class="string">test_X0 = tmp.reshape(1, len(tmp)).tolist()[0]</span></span><br><span class="line"><span class="string">X_test = tf.fit_transform(ctvecter.fit_transform(test_X0)).toarray()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对 y进行编码, 一开始用的这个，后来查文档发现是：</span></span><br><span class="line"><span class="comment"># Encode target labels with value between 0 and n_classes-1.</span></span><br><span class="line"><span class="comment"># This transformer should be used to encode target values, i.e. y, and not the input X.</span></span><br><span class="line"><span class="string">""" encoder = preprocessing.LabelEncoder()</span></span><br><span class="line"><span class="string">data_y = encoder.fit_transform(data_y)</span></span><br><span class="line"><span class="string">test_y = encoder.fit_transform(test_y) """</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 于是这样自己写的：如果不大于等于3，就替换为0……</span></span><br><span class="line">data_y = data_y.where(data_y[<span class="number">0</span>] &gt;= <span class="number">3</span>, <span class="number">0</span>).where(data_y[<span class="number">0</span>] &lt; <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">test_y = test_y.where(test_y[<span class="number">0</span>] &gt;= <span class="number">3</span>, <span class="number">0</span>).where(test_y[<span class="number">0</span>] &lt; <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">tmp = np.array(data_X)</span><br><span class="line">train_X = tmp.reshape(<span class="number">1</span>, len(tmp)).tolist()[<span class="number">0</span>]</span><br><span class="line"><span class="string">""" for i in range(565912):</span></span><br><span class="line"><span class="string">    print(train_X[i]) """</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf-idf向量</span></span><br><span class="line">tfidf = TfidfVectorizer(decode_error=<span class="string">'ignore'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">X_tfidf = tfidf.fit_transform(train_X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># logistics</span></span><br><span class="line">LR = LogisticRegression()</span><br><span class="line">LR.fit(X_tfidf, data_y)</span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">tmp = np.array(test_X)</span><br><span class="line">test_X = tmp.reshape(<span class="number">1</span>, len(tmp)).tolist()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">X_test = tfidf.transform(test_X)</span><br><span class="line"></span><br><span class="line">print(LR.score(X_test, test_y)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后得到的结果是 0.8418418，而且随着我自己切割的训练集的增大而提高</span></span><br></pre></td></tr></table></figure>



<h6 id="SVM-amp-boost-word2vec"><a href="#SVM-amp-boost-word2vec" class="headerlink" title="SVM&amp;boost+word2vec"></a>SVM&amp;boost+word2vec</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection, preprocessing, linear_model, naive_bayes, metrics, svm</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line"><span class="keyword">import</span> sys </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_X = pandas.read_csv(<span class="string">'F:/tmp/amazon/train1.tgt'</span>, sep = <span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line">data_y = pandas.read_csv(<span class="string">'F:/tmp/amazon/train1.lab'</span>, sep = <span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line">test_X = pandas.read_csv(<span class="string">'F:/tmp/amazon/test.tgt'</span>, sep = <span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line">test_y = pandas.read_csv(<span class="string">'F:/tmp/amazon/test.lab'</span>, sep = <span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 改变数据形式</span></span><br><span class="line">tmp = np.array(data_X)</span><br><span class="line">tmp = tmp.reshape(<span class="number">1</span>, len(tmp)).tolist()[<span class="number">0</span>]</span><br><span class="line">data_X = [item.split() <span class="keyword">for</span> item <span class="keyword">in</span> tmp]</span><br><span class="line"></span><br><span class="line">tmp = np.array(test_X)</span><br><span class="line">tmp = tmp.reshape(<span class="number">1</span>, len(tmp)).tolist()[<span class="number">0</span>]</span><br><span class="line">test_X = [item.split() <span class="keyword">for</span> item <span class="keyword">in</span> tmp]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一次用word2vec，测试了一下效果，感觉还好</span></span><br><span class="line"><span class="string">""" </span></span><br><span class="line"><span class="string">w2v = Word2Vec(data_X, sg=1, iter=8)</span></span><br><span class="line"><span class="string">for e in w2v.most_similar(positive=['good'], topn=10):</span></span><br><span class="line"><span class="string">    print(e[0], e[1])</span></span><br><span class="line"><span class="string">bad 0.9058005809783936</span></span><br><span class="line"><span class="string">pretty 0.8905544281005859</span></span><br><span class="line"><span class="string">really 0.8673542737960815</span></span><br><span class="line"><span class="string">great 0.8508365154266357</span></span><br><span class="line"><span class="string">but 0.8443725109100342</span></span><br><span class="line"><span class="string">very 0.8429215550422668</span></span><br><span class="line"><span class="string">decent 0.8421783447265625</span></span><br><span class="line"><span class="string">weak 0.8297692537307739</span></span><br><span class="line"><span class="string">sweet 0.8187519311904907</span></span><br><span class="line"><span class="string">convenient 0.8135256767272949 """</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_y = data_y.where(data_y[<span class="number">0</span>] &gt;= <span class="number">3</span>, <span class="number">0</span>).where(data_y[<span class="number">0</span>] &lt; <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">test_y = test_y.where(test_y[<span class="number">0</span>] &gt;= <span class="number">3</span>, <span class="number">0</span>).where(test_y[<span class="number">0</span>] &lt; <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># word2vec模型，遍历所有文档训练，得到每个词的向量</span></span><br><span class="line">w2v = Word2Vec(data_X, min_count=<span class="number">1</span>, size=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求平均得到每一句话的向量，应该还会有更好的方法</span></span><br><span class="line">X_train = []</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> data_X:</span><br><span class="line">    vec = np.zeros(<span class="number">50</span>).reshape((<span class="number">1</span>, <span class="number">50</span>))[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 一开始没加[0]，训练时报错ValueError: Found array with dim 3. Estimator expected &lt;= 2.</span></span><br><span class="line"><span class="comment"># 以为是test_y的问题，找了很久才发现是X。</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">        vec += w2v[word].reshape((<span class="number">1</span>, <span class="number">50</span>))[<span class="number">0</span>]</span><br><span class="line">    vec /= len(sentence)</span><br><span class="line">    X_train.append(vec)</span><br><span class="line"></span><br><span class="line">X_test = []</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> test_X:</span><br><span class="line">    vec = np.zeros(<span class="number">50</span>).reshape((<span class="number">1</span>, <span class="number">50</span>))[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            vec += w2v[word].reshape((<span class="number">1</span>, <span class="number">50</span>))[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 第一次尝试try，因为测试集里可能会出现新单词</span></span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    vec /= len(sentence)</span><br><span class="line">    X_test.append(vec)</span><br><span class="line"></span><br><span class="line"><span class="comment"># svm模型；0 </span></span><br><span class="line">clf = svm.SVC()</span><br><span class="line"><span class="comment"># abacf = AdaBoostClassifier() # ababoost，运行这个的时候卡了。。于是削减了一下训练数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># abacf.fit(X_tfidf, data_y)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">clf_res = clf.fit(X_train, data_y)</span><br><span class="line">print(clf_res.score(X_test, test_y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后结果都是0.848左右</span></span><br></pre></td></tr></table></figure>







<h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><p>曾经纠结先进行分割为训练集测试集再对训练集的词汇建立tf-idf模型，还是对所有的评论建立。后来觉得训练出来模型需要面对的是一个陌生的环境，所以即使有些词汇不出现也是正常的。于是就按上面的来了。</p>
<p>此外，这次没有去除stopword，只是前几次在尝试分词的时候使用了，下次项目可以尝试一下</p>
<p>此次的实验都是以尝试为主，前面耽误了太多时间，也走了很多弯路，也没有去追求效果。</p>
<p>发现python把大多数方法都进行了封装，使得困难的地方不在于设计模型，设计算法，而在于数据预处理、模型算法选择、参数选择……这样有种把精力都放在了不那么主要的地方的感觉。</p>
<p>而且，可以用一个抽象的model，然后将sklearn的方法传进去，这样可能简洁一点。</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>report</tag>
        <tag>share</tag>
        <tag>classification</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>choose_all_by_yourself</title>
    <url>/2020/02/11/choose-all-by-yourself/</url>
    <content><![CDATA[<p>choose everything by yourself, for yourself, and which makes you be yourself</p>
<p>choose friendship, choose love, choose job, choose goal, choose furture and choose youself</p>
<a id="more"></a>

<p>看完了时光慢递，这是一部本应该去年夏日就看完的片子。它仿佛来自去年的我好奇的问候一样，姗姗来迟。也好像未来的我的悄悄的叮嘱和小心翼翼的祝福。</p>
<p>看到里面那熟悉的园子，熟悉的活动，熟悉的一草一木。</p>
<p>连空气都是自由的。当然，阳光下的灰尘也是。</p>
<p>自由，诞生了思想，诞生了伟大，同时也滋生了堕落与迷醉。</p>
<p>看着每一个熟悉或者不熟悉的人，或快或慢的或东或西，而我自己却沉醉在自己的世界，一个充满琐事，充满分心，充满焦虑，充满小气的世界，仿佛突然不小心，或者偶尔，伸出了头，探到窗外瞥了一眼，看到了清新，看到了充满的希望。</p>
<p>选择，           ———</p>
<p><em>写到这里，我用半个多小时去看了一篇学长拍这个电影的始末——<a href="https://zhuanlan.zhihu.com/p/85811675" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/85811675</a></em></p>
<p><em>于是也看到了种种的无奈，看到了追逐梦想背后的烦恼与无力。</em></p>
<p>与之而来的是我忘记了看之前的那半句话想说什么。</p>
<p>可是我知道了现在想说什么：于是，我选择 纯粹。</p>
<p>为了单一的目标。只有纯粹。</p>
<p>本来还想继续分析自己的性格，自己的目标，自己的决定，自己的优缺点……</p>
<p>可是突然发现已经打鸡血似的好多次了，多说无益，看自己的行动。</p>
<p>最后也放几张图片吧，作为曾经，作为许诺，作为希冀。</p>
<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200212011212795.png" alt="image-20200212011212795" style="zoom:25%;" />



<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200212011258563.png" alt="image-20200212011258563" style="zoom:60%;" />



<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200212011334641.png" alt="image-20200212011334641" style="zoom:50%;" />



<p>在这里：</p>
<blockquote>
<p>敢去做改变自己人生轨迹的选择<br>并不是因为勇敢<br>而是因为去做自己不喜欢的事情<br>需要更大的勇气</p>
</blockquote>
<p>选择成为怎样的自己</p>
<p>最后的最后，希望能舍得一些东西，决绝一些东西，争取一些东西。</p>
<p>至此。</p>
<p>ard</p>
<p>2020.02.12.1:14凌晨</p>
<p>于既倒的flag和新立的flag旁</p>
]]></content>
      <categories>
        <category>thoughts</category>
        <category>reviews</category>
      </categories>
      <tags>
        <tag>films</tag>
        <tag>flags</tag>
      </tags>
  </entry>
  <entry>
    <title>habit_and_nature</title>
    <url>/2020/01/31/habit-and-nature/</url>
    <content><![CDATA[<p>country :  nature &amp; habit</p>
<a id="more"></a>

<h5 id="乡土中国"><a href="#乡土中国" class="headerlink" title="乡土中国"></a>乡土中国</h5><p> 《乡土中国》，如题，写的是乡土社会，中国的乡土社会。因此这个对象就有两个特点，一个是乡土的特点，我归纳为自然与习惯，另一个是中国人所带来的属性：对土地的独特感觉与受到古文化仁义礼智信的影响。</p>
<p>这两种特点相互影响相互融合，构成了实实在在有血有肉的乡土中国。</p>
<p>从这两种特点来解构书中所提及的各种现象和定义，也就不难理解了。比如乡村人对土地对泥土的尊重与依赖，认为那是根，那是魂。中国人总是自觉而不自知的去开垦，去耕种，总是把生命与精神跟土地捆绑在一起。<em>孩子在土里洗澡，爸爸在土里流汗，爷爷在土里埋葬</em>。一生都浸在土地里。</p>
<p>至于自然与习惯，不管是无讼，还是无为政治，还是所说的长老权力还是时势权力，都是很自然的情况下的一种社会反应，一种一群自觉而不自知的人聚在一起自然而然所呈现的格局。人天生而有自私自爱的心，由此自然产生了作者所说的差序格局。</p>
<blockquote>
<p>在乡土社会中欲望经了文化的陶冶可以作为行为的指导，结果是印合于生存的条件。但是这种印合并不是自觉的，并不是计划的，乡土文化中微妙的配搭可以说是天工，而非人力，虽则文化是人为的。这种不自觉的印合，有它的弊病，那就是如果环境变了，人并不能作主动的有计划的适应，只能如孙末楠所说的盲目的经过错误与试验的公式来找新的办法。乡土社会环境不很变，因之文化变迁的速率也慢，人们有时间可以从容的作盲目的试验，错误所引起的损失不会是致命的。</p>
</blockquote>
<p>我觉得这一段说明了一些文化、生存、环境的关系。</p>
<p>自然的原因是因为变化缓慢，自然便已经足够。而仅仅自然，却并不能构成乡土的文明，还需要文化的参与与调整。根深蒂固代代相传下来，便成了习惯。</p>
<blockquote>
<p>礼治在表面看去好象是人们行为不受规律拘束而自动形成的秩序。其实自动的说法是不确，只是主动的服于成规罢了。孔子一再的用“克”字，用“约”字来形容礼的养成，可见礼治并不是离开社会，由于本能或无意所构成的秩序了。</p>
<p>法治和礼治是发生在两种不同的社会情态中。这里所谓礼治也许就是普通所谓人治，但是礼治一词不会象人治一词那样容易引起误解，以致有人觉得社会秩序是可以由个人好恶来维持的了。礼治和这种个人好恶的统治相差很远，因为礼是传统，是整个社会历史在维持这种秩序。礼治社会并不能在变迁很快的时代中出现的，这是乡土社会的特色。</p>
</blockquote>
<p>无论是纲常伦理，还是仁义道德，无论是佛祖菩萨，还是神仙圣人，都深深影响了乡村的行为，并逐渐陷入，结合，成为乡村性格的一部分。并且由于乡村的缓慢，他们的影响愈发显得持久，根深蒂固而且理所应当。</p>
<p>万事皆有规矩，而规矩渐渐成为自然。乡村的一个慢字，显得晃晃悠悠，有理有据。</p>
<p>日出而作，日落而息，阡陌交通，鸡犬相闻，乡村是富足的，拥有着广袤的土地，烈火的山河，所求皆由天意，祈愿风调雨顺；所欲皆靠自己，须得春生夏长，秋收冬藏。</p>
<p>乡村中每一天，都像一年；每一年，都像一生。</p>
<p>乡土社会便像一个浅潭，看似亘古不变却自有代谢自有条理自有体系，无知无为却自觉自然。</p>
<h5 id="一些其他"><a href="#一些其他" class="headerlink" title="一些其他"></a>一些其他</h5><hr>
<p>这是第一次读社会学的作品，之前一直局限在科学文学哲学等方面。发现自己很喜欢这种理性的分析与思辨，从各种角度，抽象总结归纳社会的现象，与自然科学相比也不遑多让，与之相同的是都需要透过现象看到本质的目光。</p>
<hr>
<p>或许是自己也在乡村长大的原因，对里面的种种现象也颇为熟悉，仿佛就是在说自己身边的事情。可惜我一直被淹没在纠纷里，没能去理出这个线团的头绪，正当局也难怪。</p>
<p>不过乡村也的确是在变化的，我也看到了这种的变化。包括青年的离乡，老人的逝世，网络的到来，封建的遗存。这些变化与那个沉重的乡村碰在一起，仿佛古潭的渐渐干涸，岸边的松动，迟滞的泥痕。</p>
<hr>
<p>答应了一个朋友写一下感想，结果迟到了这么多天。果然明日复明日啊，时不待我，以此为戒。</p>
<hr>
<p>因为是看完后多天后写的，所以心中只记下了最有感触的地方，不求具体详细。</p>
<hr>
<p>ard</p>
<p>2020.2.1午后</p>
<p>家中</p>
]]></content>
      <categories>
        <category>reviews</category>
      </categories>
      <tags>
        <tag>books</tag>
        <tag>society</tag>
      </tags>
  </entry>
  <entry>
    <title>a_dogs_purpose</title>
    <url>/2020/01/26/a-dogs-purpose/</url>
    <content><![CDATA[<p>If everything is true &amp; hope everything is true</p>
<a id="more"></a>

<h5 id="自问使命"><a href="#自问使命" class="headerlink" title="自问使命"></a>自问使命</h5><p>我觉得人也应该如此，问一下自己存在的意义，自己存在的使命。</p>
<p>最后Bailey找到了自己的答案：</p>
<blockquote>
<p>So, in all my lives as a dog, here’s what I’ve learned. Have fun, Obviously. Whatever possible, find someone to save, and save them. Lick the ones you love. Don’t get all sad-faced about what happened and scrunchy-faced about what could. Okay, go get it. Be here now. That’s it. That’s a dog’s purpose.</p>
</blockquote>
<p>感觉说的很好。</p>
<p>思考一下，自己的使命是什么。好像的确是这样，生活中的大家皆纷纷攘攘，都聚焦于眼前的事情，或升学，或工作，或享受。少有人能抬起头来，望一望远处，远处的未来；低下头去，望一望深处，深处的内心。生活不仅有眼前的苟且，诗与远方，还有使命与行囊。</p>
<p>至于我自己，除了一些学业上的规划，问一下自己的内心想做的事情。是创新与改变，还有广泛而经历。</p>
<hr>
<h5 id="一些杂思"><a href="#一些杂思" class="headerlink" title="一些杂思"></a>一些杂思</h5><p>越发喜欢美国或者西欧那种阔远明绿的风景，仿佛视野开阔，心情也开阔了起来。</p>
<p>当然也依旧喜欢江南，但是似乎当下的中国，那种风景沦为旅游的名片，而不自然，不是生活。</p>
<p>希望日后的自己能到一个自己喜欢的地方</p>
<hr>
<p>原来一个人的一生可以这么轻易的被改变。橄榄球优秀，奖学金上大学，有心仪的对方，计划好的美好未来，就因为一场火，一切都改变了。一切都改变了。</p>
<hr>
<p>最后Bailey与Ethan相遇相识，即使人物皆非，但依旧在那儿。那个场景让人泪目。</p>
<p>Yes, it’s me, Ethan.</p>
<p>2020.1.26</p>
<p>为了Bailey</p>
<p>ard</p>
]]></content>
      <categories>
        <category>reviews</category>
      </categories>
      <tags>
        <tag>films</tag>
      </tags>
  </entry>
  <entry>
    <title>farewell_my_2019</title>
    <url>/2020/01/25/farewell-my-2019/</url>
    <content><![CDATA[<p>farewell &amp; welcome</p>
<a id="more"></a>

<h5 id="What-has-time-told"><a href="#What-has-time-told" class="headerlink" title="What has time told?"></a>What has time told?</h5><p>刚刚读完了2019除夕时写下的对自己想说的话，感慨颇多。</p>
<p>有很多感受：尊敬过去那个自信说出未来的自己，并继续认为那正是我的方向。也为现在并没有拼尽全力去朝着那个方向踽踽前行而愧疚，并真真切切的感受到了自己灵魂的干涩。</p>
<p>首先，过去的这个2019，是在现实中失去信仰，生活失去根基的一年。上半年开始显露端倪，暑假时痛定思痛稍微改善，然后下学期初踌躇满志，最后彻底堕落与沦陷。以至于接近万劫不复。</p>
<p>表现有很多，对自己要求不断降低，美其名曰知足常乐。对自己的挫折故意逃避，美其名曰不为过去悲伤，只看将来。不停看手机，刷碎片，美其名曰放松消遣，与时俱进，融入大家。对自己的努力夸大其词，美其名曰拼搏努力，不懈奋斗，自我感动。过分依赖他人指点与陪伴，美其名曰有知心朋友，共同进步。抄袭应付作业形成习惯，美其名曰抓紧时间，充分利用时间复习更好掌握。生活不条理不在意，美其名曰在努力学习无暇收拾整理。</p>
<p>于是，我在逐渐失去自己。 </p>
<p>失去那个稍微有些自负，有些情怀，有些学问，有些自持，有些想法，有些自我，有些谨慎，有些自律的那个我。</p>
<p>结果可想而知。</p>
<p>而且这个过程，许多人都感觉到了。其实，连自己也感觉到了，但是不敢面对，或者说是懒于面对。</p>
<p>当然，2019也收获了一些。去结识了更多的人，领导了一个个的活动，赚到了人生第一桶金，遇到了那个人，选择了实验室，确定了近几年的未来。也收获了更重要的 惨痛 的教训。</p>
<p>farewell 2019. </p>
<p>对farewell这个词一直很感兴趣，于是去查了一下它的词源</p>
<blockquote>
<p>late 14c., from Middle English faren wel, verbal phrase attested by c.1200 (see fare (v.) + well (adv.)); usually said to the departing person, who replied with good-bye. As a noun, by early 15c.</p>
</blockquote>
<p>原来fare不仅有费用，还有吃、住、行的意思。真好，愿你一切都好。</p>
<p>心已经笃定，脚步迈出，未来坚实。</p>
<p>farewell 2019！</p>
<h5 id="time-will-tell"><a href="#time-will-tell" class="headerlink" title="time will tell"></a>time will tell</h5><p>20，弱冠。</p>
<blockquote>
<p>“二十曰弱，冠。”  ——《礼记．曲礼上》</p>
<p>“二十成人，初加冠，体犹未壮，故曰弱也。” ——《孔颖达．正义》</p>
<p>“冠者，礼之始也。”  ——《礼记．冠义》</p>
<p>“男子二十，冠而字。”  ——《礼记．曲礼上》</p>
<p>“已冠而字之，成人之道也。” ——《礼记．冠义》</p>
</blockquote>
<p>不知不觉20了，一直不愿或者回避长大，因为不想去接触那些复杂的或者繁琐的或者不得不的那些东西。</p>
<p>然而我现在不再那样了。我开始接受，并将坦然面对，面对那些礼节，那些责任，那些不得不。并将坚持一些自己的东西，宁愿被讽，被骂，被难以理解。</p>
<p>看到一篇回答，”人是如何变得平庸的“，感慨颇多，其中有很多现象我深以为然。里面也有一句话：</p>
<blockquote>
<p>人废掉的本质很简单——长期沉迷于消耗性事物，而很少做创造性的事情。</p>
</blockquote>
<p>我应该去做创造性的事情。</p>
<p>不应让自己满足。去多逼迫一下自己吧。</p>
<blockquote>
<p>时间就像海绵里的水，只要愿意挤，总还是有的</p>
</blockquote>
<p>长大 &amp; 进步</p>
<p>相信自己，知道自己是谁，时时刻刻。</p>
<p>不为了融入淤泥而自染。</p>
<p>洗涤风尘，还自己初心本来面目</p>
<p>看到一句话，很喜欢：</p>
<blockquote>
<p>下决心要做的事，要保持不动声色却满心澎湃</p>
</blockquote>
<p>自勉</p>
<p>welcome 2020！</p>
<p>除夕夜思</p>
<p>初一晨写</p>
<p>于书桌，熟睡父亲身旁</p>
<p>ard</p>
<p>2020.1.25.2:59</p>
]]></content>
      <categories>
        <category>thought</category>
      </categories>
      <tags>
        <tag>summary</tag>
        <tag>plan</tag>
      </tags>
  </entry>
  <entry>
    <title>84,Charing_Cross_Road</title>
    <url>/2020/01/19/84-Charing-Cross-Road/</url>
    <content><![CDATA[<p>my first book review</p>
<a id="more"></a>

<h4 id="查令十字街84号："><a href="#查令十字街84号：" class="headerlink" title="查令十字街84号："></a>查令十字街84号：</h4><p>​    当弗兰克多年后仍坐在店门左手边的那张书桌上，笔尖跳跃着书写的时候，会不会想起他第一次打开那浅黄色信笺的那个清晨？</p>
<blockquote>
<p>诸位先生：</p>
<p>　　我在《星期六文学评论》上看到你们刊登的广告，上头说你们“专营绝版书”。另一个字眼“古书商”总是令我望之却步，因为我总认为：既然“古”，一定也很“贵”吧。而我只不过是一名对书本有着“古老”胃口的穷作家罢了。在我住的地方，总买不到我想读的书，要不是索价奇昂的珍本，就是巴诺书店里头那些被小鬼涂得乱七八糟的邋遢书。</p>
<p>　　随信附上一份清单，上面列出我目前最想读而又遍寻不着的书。如果贵店有符合该书单所列，而每本又不高于五美元的话，可否径将此函视为订购单，并将书寄给我？</p>
<p>你忠实的</p>
<p>海莲·汉芙(小姐)</p>
</blockquote>
<p>​    </p>
<p>​    这是一个开始于1949年10月的故事。</p>
<p>​    如果说，贝克街221号B座是一些人心中敬畏的圣地，是辨识同道的暗号，是谈论时不必明言的默契，那么查令十字街84号就是另一些人魂牵梦萦远方的回音，是天地悠悠而不必怆然的陪伴，是虽在梦里虽在远方虽在一隅的心安。任一个地方拥有其中的一处即是上天的眷恋，伦敦何德，令人心向往之。</p>
<p>​    翻开这本书，看着一封封往来的信件，我看到了岁月。他们因书结缘，这是多么美丽的邂逅！想必谁也不会预料到，这段人与书、书与人、人与人的烟云缭绕会持续20年。</p>
<p>​    由”诸位先生”到”FPD”到直呼其名，再由”大懒虫”到”亲爱的急惊风”，汉芙将心情诉诸笔端。调侃、抱怨，感激、责怪，关心、问候，无话不可以谈给遥远的大洋彼岸灯下的那人。与其说是写在纸上，不如说是说与海听。</p>
<p>​    那个一直未予谋面的绅士，一直默默在昏黄的灯下，在旧书堆里，在一个个清晨，一个个傍晚，一个个深夜，为那位未予谋面的”亲爱的汉芙小姐”回信。无论是来信的抱怨，来信的感激，还是来信的调侃，都一丝不苟的写回。</p>
<p>​    缘起于书，未止于书。开始互相理解，互相倾诉。但双方并未像小说一样如何如何，这很生活。始于平淡，终于平淡。这就是真实。我也相信生活中的大多数心动，也会藏在书卷中，纸页下，随时间，任岁月，淘洗墨痕，唯风指翻书时，宛若昨天。</p>
<p>​    双方都是爱书的人，在我心中，书，同样是一个美丽的字。”半床明月半床书”，是我最向往的生活。小时候爱书到痴，情愿失去所有，但教可以睁眼是书，闭眼是书，天天与书为伴，那便在我心中是神仙也比不过的日子了。</p>
<p>​    爱书的人，总是善的。</p>
<p>​    被感动，知道了那一处：查令十字街84号，那个灵魂合奏的地方。</p>
<p>​    如是。</p>
<p>​    过去。</p>
<p>​    那时候，车、马、邮件都慢，慢到可以静下心来，也慢到更加珍惜。</p>
<p>​    见字如面……</p>
<p>​    ard</p>
<p>​    2020.1.19日夜于火炉旁</p>
]]></content>
      <categories>
        <category>reviews</category>
      </categories>
      <tags>
        <tag>books</tag>
        <tag>novels</tag>
      </tags>
  </entry>
  <entry>
    <title>first-try</title>
    <url>/2020/01/09/first-try/</url>
    <content><![CDATA[<p>This is a drop of sea</p>
<a id="more"></a>



<h5 id="tag"><a href="#tag" class="headerlink" title="tag"></a>tag</h5><p>“- try”</p>
<h5 id="categories"><a href="#categories" class="headerlink" title="categories"></a>categories</h5><p>“- hello? world!”</p>
<h5 id="url"><a href="#url" class="headerlink" title="url"></a>url</h5><p>““??</p>
<img src="/2020/01/09/first-try/082.jpg" class="" title="This is an example image">

<p><img src="082.jpg" alt=""></p>
<p>why there is no pic?</p>
]]></content>
      <categories>
        <category>hello? world!</category>
      </categories>
      <tags>
        <tag>try md</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/01/09/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. </p>
<a id="more"></a>

<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
</search>
